{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sig_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_000644bb2</th>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_000779bfc</th>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_000a6266a</th>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_0015fd391</th>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_001626bd3</th>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_fffb1ceed</th>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_fffb70c0c</th>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_fffc1c3f4</th>\n",
       "      <td>ctl_vehicle</td>\n",
       "      <td>48</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>-0.2541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_fffcb9e7c</th>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_ffffdd77b</th>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 875 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  cp_type  cp_time cp_dose     g-0     g-1     g-2     g-3  \\\n",
       "sig_id                                                                       \n",
       "id_000644bb2       trt_cp       24      D1  1.0620  0.5577 -0.2479 -0.6208   \n",
       "id_000779bfc       trt_cp       72      D1  0.0743  0.4087  0.2991  0.0604   \n",
       "id_000a6266a       trt_cp       48      D1  0.6280  0.5817  1.5540 -0.0764   \n",
       "id_0015fd391       trt_cp       48      D1 -0.5138 -0.2491 -0.2656  0.5288   \n",
       "id_001626bd3       trt_cp       72      D2 -0.3254 -0.4009  0.9700  0.6919   \n",
       "...                   ...      ...     ...     ...     ...     ...     ...   \n",
       "id_fffb1ceed       trt_cp       24      D2  0.1394 -0.0636 -0.1112 -0.5080   \n",
       "id_fffb70c0c       trt_cp       24      D2 -1.3260  0.3478 -0.3743  0.9905   \n",
       "id_fffc1c3f4  ctl_vehicle       48      D2  0.3942  0.3756  0.3109 -0.7389   \n",
       "id_fffcb9e7c       trt_cp       24      D1  0.6660  0.2324  0.4392  0.2044   \n",
       "id_ffffdd77b       trt_cp       72      D1 -0.8598  1.0240 -0.1361  0.7952   \n",
       "\n",
       "                 g-4     g-5     g-6  ...    c-90    c-91    c-92    c-93  \\\n",
       "sig_id                                ...                                   \n",
       "id_000644bb2 -0.1944 -1.0120 -1.0220  ...  0.2862  0.2584  0.8076  0.5523   \n",
       "id_000779bfc  1.0190  0.5207  0.2341  ... -0.4265  0.7543  0.4708  0.0230   \n",
       "id_000a6266a -0.0323  1.2390  0.1715  ... -0.7250 -0.6297  0.6103  0.0223   \n",
       "id_0015fd391  4.0620 -0.8095 -1.9590  ... -2.0990 -0.6441 -5.6300 -1.3780   \n",
       "id_001626bd3  1.4180 -0.8244 -0.2800  ...  0.0042  0.0048  0.6670  1.0690   \n",
       "...              ...     ...     ...  ...     ...     ...     ...     ...   \n",
       "id_fffb1ceed -0.4713  0.7201  0.5773  ...  0.1969  0.0262 -0.8121  0.3434   \n",
       "id_fffb70c0c -0.7178  0.6621 -0.2252  ...  0.4286  0.4426  0.0423 -0.3195   \n",
       "id_fffc1c3f4  0.5505 -0.0159 -0.2541  ...  0.5409  0.3755  0.7343  0.2807   \n",
       "id_fffcb9e7c  0.8531 -0.0343  0.0323  ... -0.1105  0.4258 -0.2012  0.1506   \n",
       "id_ffffdd77b -0.3611 -3.6750 -1.2420  ... -3.3890 -1.7450 -6.6300 -4.0950   \n",
       "\n",
       "                c-94    c-95    c-96    c-97    c-98    c-99  \n",
       "sig_id                                                        \n",
       "id_000644bb2 -0.1912  0.6584 -0.3981  0.2139  0.3801  0.4176  \n",
       "id_000779bfc  0.2957  0.4899  0.1522  0.1241  0.6077  0.7371  \n",
       "id_000a6266a -1.3240 -0.3174 -0.6417 -0.2187 -1.4080  0.6931  \n",
       "id_0015fd391 -0.8632 -1.2880 -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "id_001626bd3  0.5523 -0.3031  0.1094  0.2885 -0.3786  0.7125  \n",
       "...              ...     ...     ...     ...     ...     ...  \n",
       "id_fffb1ceed  0.5372 -0.3246  0.0631  0.9171  0.5258  0.4680  \n",
       "id_fffb70c0c -0.8086 -0.9798 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "id_fffc1c3f4  0.4116  0.6422  0.2256  0.7592  0.6656  0.3808  \n",
       "id_fffcb9e7c  1.5230  0.7101  0.1732  0.7015 -0.6290  0.0740  \n",
       "id_ffffdd77b -7.3860 -1.4160 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[23814 rows x 875 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = pd.read_csv('train_features.csv',index_col='sig_id')\n",
    "df_test_features = pd.read_csv('test_features.csv',index_col='sig_id')\n",
    "df_sample_submission = pd.read_csv('sample_submission.csv', index_col='sig_id')\n",
    "df_targets = pd.read_csv('train_targets_scored.csv',index_col='sig_id')\n",
    "df_targets_nonscored = pd.read_csv('train_targets_nonscored.csv',index_col='sig_id')\n",
    "df = pd.concat([df_features, df_targets], axis=1)\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_df(df):\n",
    "    df['cp_type'] = df['cp_type'].apply(lambda x: 1 if x == \"ctl_vehicle\" else 0)\n",
    "    df['cp_dose'] = df['cp_dose'].apply(lambda x: 1 if x == \"D2\" else 0)\n",
    "    df['cp_time'] = df['cp_time'].apply(lambda x: (x - 24)/48)\n",
    "    return df\n",
    "df_features = change_df(df_features)\n",
    "df_test_features = change_df(df_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets = df_targets[df_features['cp_type'] == 0]\n",
    "df_targets_nonscored = df_targets_nonscored[df_features['cp_type'] == 0]\n",
    "df_features = df_features[df_features['cp_type'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl\n",
      "Collecting pytorch-tabnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Url '/kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "ERROR: Could not install packages due to an EnvironmentError: Invalid URL '/kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl': No schema supplied. Perhaps you meant http:///kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl?\n",
      "\n",
      "ERROR: Invalid requirement: '../input/autograd'\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl pytorch-tabnet\n",
    "!pip install ../input/autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iterative-stratification in c:\\users\\sanraj\\anaconda3\\envs\\fastai\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sanraj\\anaconda3\\envs\\fastai\\lib\\site-packages (from iterative-stratification) (0.23.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sanraj\\anaconda3\\envs\\fastai\\lib\\site-packages (from iterative-stratification) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sanraj\\anaconda3\\envs\\fastai\\lib\\site-packages (from iterative-stratification) (1.18.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sanraj\\anaconda3\\envs\\fastai\\lib\\site-packages (from scikit-learn->iterative-stratification) (2.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sanraj\\anaconda3\\envs\\fastai\\lib\\site-packages (from scikit-learn->iterative-stratification) (0.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"iterative-stratification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabnet\n",
      "  Using cached pytorch_tabnet-2.0.1-py3-none-any.whl (30 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch<2.0,>=1.2 (from pytorch-tabnet) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)\n",
      "ERROR: No matching distribution found for torch<2.0,>=1.2 (from pytorch-tabnet)\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_tabnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-371905409a36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/iterative-stratification/iterative-stratification-master'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0miterstrat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml_stratifiers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultilabelStratifiedKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpytorch_tabnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtab_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTabNetRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTqdmCallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_tabnet'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from tqdm import *\n",
    "from tqdm.keras import TqdmCallback\n",
    "from imblearn.over_sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_features.values\n",
    "y = df_targets.values\n",
    "y_nonscored = df_targets_nonscored.values\n",
    "X_test = df_test_features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_g = PCA(n_components=50)\n",
    "\n",
    "X_pca_g = pca_g.fit_transform(X[:, 3:-100])\n",
    "X_test_pca_g = pca_g.transform(X_test[:, 3:-100])\n",
    "\n",
    "pca_c = PCA(n_components=15)\n",
    "\n",
    "X_pca_c = pca_c.fit_transform(X[:, -100:])\n",
    "X_test_pca_c = pca_c.transform(X_test[:, -100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "X_stats_1 = stats.describe(X[:, 3:-100], axis=1)\n",
    "X_stats_2 = stats.describe(X[:, -100:], axis=1)\n",
    "X_stats_3 = stats.describe(X[:, 3:], axis=1)\n",
    "X_stats = np.vstack((X_stats_1.mean, X_stats_1.variance, X_stats_1.skewness, X_stats_1.kurtosis, X_stats_2.mean, X_stats_2.variance, X_stats_2.skewness, X_stats_2.kurtosis, X_stats_3.mean, X_stats_3.variance, X_stats_3.skewness, X_stats_3.kurtosis)).T\n",
    "X_stats_1 = stats.describe(X_test[:, 3:-100], axis=1)\n",
    "X_stats_2 = stats.describe(X_test[:, -100:], axis=1)\n",
    "X_stats_3 = stats.describe(X_test[:, 3:], axis=1)\n",
    "X_test_stats = np.vstack((X_stats_1.mean, X_stats_1.variance, X_stats_1.skewness, X_stats_1.kurtosis, X_stats_2.mean, X_stats_2.variance, X_stats_2.skewness, X_stats_2.kurtosis, X_stats_3.mean, X_stats_3.variance, X_stats_3.skewness, X_stats_3.kurtosis)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=0.5)\n",
    "X = np.concatenate([X[:, 1:3], selector.fit_transform(X[:, 3:])], axis=1)\n",
    "X_test = np.concatenate([X_test[:, 1:3], selector.transform(X_test[:, 3:])], axis=1)\n",
    "\n",
    "#X = X[:, top_feats]\n",
    "#X_test = X_test[:, top_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-46489fd706be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQuantileTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_distribution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"normal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(X.shape[1]-2)):\n",
    "    scaler = QuantileTransformer(output_distribution=\"normal\")\n",
    "    X[:, i+2] = scaler.fit_transform(X[:, i+2].reshape(-1, 1)).reshape(-1)\n",
    "    X_test[:, i+2] = scaler.transform(X_test[:, i+2].reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X, X_stats, X_pca_g, X_pca_c], axis=1)\n",
    "X_test = np.concatenate([X_test, X_test_stats, X_test_pca_g, X_test_pca_c], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float).to(device)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "class MoATestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = torch.tensor(features, dtype=torch.float).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookahead implementation from https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/lookahead.py\n",
    "\n",
    "\"\"\" Lookahead Optimizer Wrapper.\n",
    "Implementation modified from: https://github.com/alphadl/lookahead.pytorch\n",
    "Paper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim import Adam\n",
    "from collections import defaultdict\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults = base_optimizer.defaults\n",
    "        self.defaults.update(defaults)\n",
    "        self.state = defaultdict(dict)\n",
    "        # manually add our defaults to the param groups\n",
    "        for name, default in defaults.items():\n",
    "            for group in self.param_groups:\n",
    "                group.setdefault(name, default)\n",
    "\n",
    "    def update_slow(self, group):\n",
    "        for fast_p in group[\"params\"]:\n",
    "            if fast_p.grad is None:\n",
    "                continue\n",
    "            param_state = self.state[fast_p]\n",
    "            if 'slow_buffer' not in param_state:\n",
    "                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n",
    "                param_state['slow_buffer'].copy_(fast_p.data)\n",
    "            slow = param_state['slow_buffer']\n",
    "            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n",
    "            fast_p.data.copy_(slow)\n",
    "\n",
    "    def sync_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update_slow(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        # print(self.k)\n",
    "        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n",
    "        loss = self.base_optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            group['lookahead_step'] += 1\n",
    "            if group['lookahead_step'] % group['lookahead_k'] == 0:\n",
    "                self.update_slow(group)\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.base_optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict['state']\n",
    "        param_groups = fast_state_dict['param_groups']\n",
    "        return {\n",
    "            'state': fast_state,\n",
    "            'slow_state': slow_state,\n",
    "            'param_groups': param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        fast_state_dict = {\n",
    "            'state': state_dict['state'],\n",
    "            'param_groups': state_dict['param_groups'],\n",
    "        }\n",
    "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
    "\n",
    "        # We want to restore the slow state, but share param_groups reference\n",
    "        # with base_optimizer. This is a bit redundant but least code\n",
    "        slow_state_new = False\n",
    "        if 'slow_state' not in state_dict:\n",
    "            print('Loading state_dict from optimizer without Lookahead applied.')\n",
    "            state_dict['slow_state'] = defaultdict(dict)\n",
    "            slow_state_new = True\n",
    "        slow_state_dict = {\n",
    "            'state': state_dict['slow_state'],\n",
    "            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n",
    "        if slow_state_new:\n",
    "            # reapply defaults to catch missing lookahead specific ones\n",
    "            for name, default in self.defaults.items():\n",
    "                for group in self.param_groups:\n",
    "                    group.setdefault(name, default)\n",
    "\n",
    "def LookaheadAdam(params, alpha=0.5, k=6, *args, **kwargs):\n",
    "     adam = Adam(params, *args, **kwargs)\n",
    "     return Lookahead(adam, alpha, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, criterion, dataloader):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        #X_batch, y_batch = [t.to(device) for t in data]\n",
    "        X_batch, y_batch = data\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs, targets = X_batch, y_batch\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        scheduler.step()\n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def validate_model(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        #X_batch, y_batch = [t.to(device) for t in data]        \n",
    "        X_batch, y_batch = data\n",
    "        \n",
    "        inputs, targets = X_batch, y_batch\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in tqdm(dataloader):\n",
    "        inputs = data\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from torch.autograd import Function\n",
    "from collections import OrderedDict\n",
    "from torch.jit import script\n",
    "from warnings import warn\n",
    "import os\n",
    "import glob\n",
    "import hashlib\n",
    "import gc\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def to_one_hot(y, depth=None):\n",
    "    r\"\"\"\n",
    "    Takes integer with n dims and converts it to 1-hot representation with n + 1 dims.\n",
    "    The n+1'st dimension will have zeros everywhere but at y'th index, where it will be equal to 1.\n",
    "    Args:\n",
    "        y: input integer (IntTensor, LongTensor or Variable) of any shape\n",
    "        depth (int):  the size of the one hot dimension\n",
    "    \"\"\"\n",
    "    y_flat = y.to(torch.int64).view(-1, 1)\n",
    "    depth = depth if depth is not None else int(torch.max(y_flat)) + 1\n",
    "    y_one_hot = torch.zeros(y_flat.size()[0], depth, device=y.device).scatter_(1, y_flat, 1)\n",
    "    y_one_hot = y_one_hot.view(*(tuple(y.shape) + (-1,)))\n",
    "    return y_one_hot\n",
    "\n",
    "\n",
    "def _make_ix_like(input, dim=0):\n",
    "    d = input.size(dim)\n",
    "    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n",
    "    view = [1] * input.dim()\n",
    "    view[0] = -1\n",
    "    return rho.view(view).transpose(0, dim)\n",
    "\n",
    "\n",
    "class SparsemaxFunction(Function):\n",
    "    \"\"\"\n",
    "    An implementation of sparsemax (Martins & Astudillo, 2016). See\n",
    "    :cite:`DBLP:journals/corr/MartinsA16` for detailed description.\n",
    "    By Ben Peters and Vlad Niculae\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, dim=-1):\n",
    "        \"\"\"sparsemax: normalizing sparse transform (a la softmax)\n",
    "        Parameters:\n",
    "            input (Tensor): any shape\n",
    "            dim: dimension along which to apply sparsemax\n",
    "        Returns:\n",
    "            output (Tensor): same shape as input\n",
    "        \"\"\"\n",
    "        ctx.dim = dim\n",
    "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
    "        input -= max_val  # same numerical stability trick as for softmax\n",
    "        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
    "        output = torch.clamp(input - tau, min=0)\n",
    "        ctx.save_for_backward(supp_size, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output = ctx.saved_tensors\n",
    "        dim = ctx.dim\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n",
    "        v_hat = v_hat.unsqueeze(dim)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        \"\"\"Sparsemax building block: compute the threshold\n",
    "        Args:\n",
    "            input: any dimension\n",
    "            dim: dimension along which to apply the sparsemax\n",
    "        Returns:\n",
    "            the threshold value\n",
    "        \"\"\"\n",
    "\n",
    "        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "        input_cumsum = input_srt.cumsum(dim) - 1\n",
    "        rhos = _make_ix_like(input, dim)\n",
    "        support = rhos * input_srt > input_cumsum\n",
    "\n",
    "        support_size = support.sum(dim=dim).unsqueeze(dim)\n",
    "        tau = input_cumsum.gather(dim, support_size - 1)\n",
    "        tau /= support_size.to(input.dtype)\n",
    "        return tau, support_size\n",
    "\n",
    "\n",
    "sparsemax = lambda input, dim=-1: SparsemaxFunction.apply(input, dim)\n",
    "sparsemoid = lambda input: (0.5 * input + 0.5).clamp_(0, 1)\n",
    "\n",
    "\n",
    "class Entmax15Function(Function):\n",
    "    \"\"\"\n",
    "    An implementation of exact Entmax with alpha=1.5 (B. Peters, V. Niculae, A. Martins). See\n",
    "    :cite:`https://arxiv.org/abs/1905.05702 for detailed description.\n",
    "    Source: https://github.com/deep-spin/entmax\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, dim=-1):\n",
    "        ctx.dim = dim\n",
    "\n",
    "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
    "        input = input - max_val  # same numerical stability trick as for softmax\n",
    "        input = input / 2  # divide by 2 to solve actual Entmax\n",
    "\n",
    "        tau_star, _ = Entmax15Function._threshold_and_support(input, dim)\n",
    "        output = torch.clamp(input - tau_star, min=0) ** 2\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        Y, = ctx.saved_tensors\n",
    "        gppr = Y.sqrt()  # = 1 / g'' (Y)\n",
    "        dX = grad_output * gppr\n",
    "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\n",
    "        q = q.unsqueeze(ctx.dim)\n",
    "        dX -= q * gppr\n",
    "        return dX, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        Xsrt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "\n",
    "        rho = _make_ix_like(input, dim)\n",
    "        mean = Xsrt.cumsum(dim) / rho\n",
    "        mean_sq = (Xsrt ** 2).cumsum(dim) / rho\n",
    "        ss = rho * (mean_sq - mean ** 2)\n",
    "        delta = (1 - ss) / rho\n",
    "\n",
    "        # NOTE this is not exactly the same as in reference algo\n",
    "        # Fortunately it seems the clamped values never wrongly\n",
    "        # get selected by tau <= sorted_z. Prove this!\n",
    "        delta_nz = torch.clamp(delta, 0)\n",
    "        tau = mean - torch.sqrt(delta_nz)\n",
    "\n",
    "        support_size = (tau <= Xsrt).sum(dim).unsqueeze(dim)\n",
    "        tau_star = tau.gather(dim, support_size - 1)\n",
    "        return tau_star, support_size\n",
    "\n",
    "\n",
    "class Entmoid15(Function):\n",
    "    \"\"\" A highly optimized equivalent of labda x: Entmax15([x, 0]) \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = Entmoid15._forward(input)\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    @script\n",
    "    def _forward(input):\n",
    "        input, is_pos = abs(input), input >= 0\n",
    "        tau = (input + torch.sqrt(F.relu(8 - input ** 2))) / 2\n",
    "        tau.masked_fill_(tau <= input, 2.0)\n",
    "        y_neg = 0.25 * F.relu(tau - input, inplace=True) ** 2\n",
    "        return torch.where(is_pos, 1 - y_neg, y_neg)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return Entmoid15._backward(ctx.saved_tensors[0], grad_output)\n",
    "\n",
    "    @staticmethod\n",
    "    @script\n",
    "    def _backward(output, grad_output):\n",
    "        gppr0, gppr1 = output.sqrt(), (1 - output).sqrt()\n",
    "        grad_input = grad_output * gppr0\n",
    "        q = grad_input / (gppr0 + gppr1)\n",
    "        grad_input -= q * gppr0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "entmax15 = lambda input, dim=-1: Entmax15Function.apply(input, dim)\n",
    "entmoid15 = Entmoid15.apply\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "\n",
    "\n",
    "class ModuleWithInit(nn.Module):\n",
    "    \"\"\" Base class for pytorch module with data-aware initializer on first batch \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._is_initialized_tensor = nn.Parameter(torch.tensor(0, dtype=torch.uint8), requires_grad=False)\n",
    "        self._is_initialized_bool = None\n",
    "        # Note: this module uses a separate flag self._is_initialized so as to achieve both\n",
    "        # * persistence: is_initialized is saved alongside model in state_dict\n",
    "        # * speed: model doesn't need to cache\n",
    "        # please DO NOT use these flags in child modules\n",
    "\n",
    "    def initialize(self, *args, **kwargs):\n",
    "        \"\"\" initialize module tensors using first batch of data \"\"\"\n",
    "        raise NotImplementedError(\"Please implement \")\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self._is_initialized_bool is None:\n",
    "            self._is_initialized_bool = bool(self._is_initialized_tensor.item())\n",
    "        if not self._is_initialized_bool:\n",
    "            self.initialize(*args, **kwargs)\n",
    "            self._is_initialized_tensor.data[...] = 1\n",
    "            self._is_initialized_bool = True\n",
    "        return super().__call__(*args, **kwargs)\n",
    "\n",
    "def download(url, filename, delete_if_interrupted=True, chunk_size=4096):\n",
    "    \"\"\" saves file from url to filename with a fancy progressbar \"\"\"\n",
    "    try:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            print(\"Downloading {} > {}\".format(url, filename))\n",
    "            response = requests.get(url, stream=True)\n",
    "            total_length = response.headers.get('content-length')\n",
    "\n",
    "            if total_length is None:  # no content length header\n",
    "                f.write(response.content)\n",
    "            else:\n",
    "                total_length = int(total_length)\n",
    "                with tqdm(total=total_length) as progressbar:\n",
    "                    for data in response.iter_content(chunk_size=chunk_size):\n",
    "                        if data:  # filter-out keep-alive chunks\n",
    "                            f.write(data)\n",
    "                            progressbar.update(len(data))\n",
    "    except Exception as e:\n",
    "        if delete_if_interrupted:\n",
    "            print(\"Removing incomplete download {}.\".format(filename))\n",
    "            os.remove(filename)\n",
    "        raise e\n",
    "    return filename\n",
    "\n",
    "\n",
    "def iterate_minibatches(*tensors, batch_size, shuffle=True, epochs=1,\n",
    "                        allow_incomplete=True, callback=lambda x:x):\n",
    "    indices = np.arange(len(tensors[0]))\n",
    "    upper_bound = int((np.ceil if allow_incomplete else np.floor) (len(indices) / batch_size)) * batch_size\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for batch_start in callback(range(0, upper_bound, batch_size)):\n",
    "            batch_ix = indices[batch_start: batch_start + batch_size]\n",
    "            batch = [tensor[batch_ix] for tensor in tensors]\n",
    "            yield batch if len(tensors) > 1 else batch[0]\n",
    "        epoch += 1\n",
    "        if epoch >= epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "def process_in_chunks(function, *args, batch_size, out=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes output by applying batch-parallel function to large data tensor in chunks\n",
    "    :param function: a function(*[x[indices, ...] for x in args]) -> out[indices, ...]\n",
    "    :param args: one or many tensors, each [num_instances, ...]\n",
    "    :param batch_size: maximum chunk size processed in one go\n",
    "    :param out: memory buffer for out, defaults to torch.zeros of appropriate size and type\n",
    "    :returns: function(data), computed in a memory-efficient way\n",
    "    \"\"\"\n",
    "    total_size = args[0].shape[0]\n",
    "    first_output = function(*[x[0: batch_size] for x in args])\n",
    "    output_shape = (total_size,) + tuple(first_output.shape[1:])\n",
    "    if out is None:\n",
    "        out = torch.zeros(*output_shape, dtype=first_output.dtype, device=first_output.device,\n",
    "                          layout=first_output.layout, **kwargs)\n",
    "\n",
    "    out[0: batch_size] = first_output\n",
    "    for i in range(batch_size, total_size, batch_size):\n",
    "        batch_ix = slice(i, min(i + batch_size, total_size))\n",
    "        out[batch_ix] = function(*[x[batch_ix] for x in args])\n",
    "    return out\n",
    "\n",
    "\n",
    "def check_numpy(x):\n",
    "    \"\"\" Makes sure x is a numpy array \"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x)\n",
    "    assert isinstance(x, np.ndarray)\n",
    "    return x\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def nop_ctx():\n",
    "    yield None\n",
    "\n",
    "\n",
    "def get_latest_file(pattern):\n",
    "    list_of_files = glob.glob(pattern) # * means all if need specific format then *.csv\n",
    "    assert len(list_of_files) > 0, \"No files found: \" + pattern\n",
    "    return max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "\n",
    "def md5sum(fname):\n",
    "    \"\"\" Computes mdp checksum of a file \"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "\n",
    "def free_memory(sleep_time=0.1):\n",
    "    \"\"\" Black magic function to free torch memory and some jupyter whims \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "def to_float_str(element):\n",
    "    try:\n",
    "        return str(float(element))\n",
    "    except ValueError:\n",
    "        return element\n",
    "    \n",
    "class ODST(ModuleWithInit):\n",
    "    def __init__(self, in_features, num_trees, depth=6, tree_dim=1, flatten_output=True,\n",
    "                 choice_function=sparsemax, bin_function=sparsemoid,\n",
    "                 initialize_response_=nn.init.normal_, initialize_selection_logits_=nn.init.uniform_,\n",
    "                 threshold_init_beta=1.0, threshold_init_cutoff=1.0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Oblivious Differentiable Sparsemax Trees. http://tinyurl.com/odst-readmore\n",
    "        One can drop (sic!) this module anywhere instead of nn.Linear\n",
    "        :param in_features: number of features in the input tensor\n",
    "        :param num_trees: number of trees in this layer\n",
    "        :param tree_dim: number of response channels in the response of individual tree\n",
    "        :param depth: number of splits in every tree\n",
    "        :param flatten_output: if False, returns [..., num_trees, tree_dim],\n",
    "            by default returns [..., num_trees * tree_dim]\n",
    "        :param choice_function: f(tensor, dim) -> R_simplex computes feature weights s.t. f(tensor, dim).sum(dim) == 1\n",
    "        :param bin_function: f(tensor) -> R[0, 1], computes tree leaf weights\n",
    "        :param initialize_response_: in-place initializer for tree output tensor\n",
    "        :param initialize_selection_logits_: in-place initializer for logits that select features for the tree\n",
    "        both thresholds and scales are initialized with data-aware init (or .load_state_dict)\n",
    "        :param threshold_init_beta: initializes threshold to a q-th quantile of data points\n",
    "            where q ~ Beta(:threshold_init_beta:, :threshold_init_beta:)\n",
    "            If this param is set to 1, initial thresholds will have the same distribution as data points\n",
    "            If greater than 1 (e.g. 10), thresholds will be closer to median data value\n",
    "            If less than 1 (e.g. 0.1), thresholds will approach min/max data values.\n",
    "        :param threshold_init_cutoff: threshold log-temperatures initializer, \\in (0, inf)\n",
    "            By default(1.0), log-remperatures are initialized in such a way that all bin selectors\n",
    "            end up in the linear region of sparse-sigmoid. The temperatures are then scaled by this parameter.\n",
    "            Setting this value > 1.0 will result in some margin between data points and sparse-sigmoid cutoff value\n",
    "            Setting this value < 1.0 will cause (1 - value) part of data points to end up in flat sparse-sigmoid region\n",
    "            For instance, threshold_init_cutoff = 0.9 will set 10% points equal to 0.0 or 1.0\n",
    "            Setting this value > 1.0 will result in a margin between data points and sparse-sigmoid cutoff value\n",
    "            All points will be between (0.5 - 0.5 / threshold_init_cutoff) and (0.5 + 0.5 / threshold_init_cutoff)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.depth, self.num_trees, self.tree_dim, self.flatten_output = depth, num_trees, tree_dim, flatten_output\n",
    "        self.choice_function, self.bin_function = choice_function, bin_function\n",
    "        self.threshold_init_beta, self.threshold_init_cutoff = threshold_init_beta, threshold_init_cutoff\n",
    "\n",
    "        self.response = nn.Parameter(torch.zeros([num_trees, tree_dim, 2 ** depth]), requires_grad=True)\n",
    "        initialize_response_(self.response)\n",
    "\n",
    "        self.feature_selection_logits = nn.Parameter(\n",
    "            torch.zeros([in_features, num_trees, depth]), requires_grad=True\n",
    "        )\n",
    "        initialize_selection_logits_(self.feature_selection_logits)\n",
    "\n",
    "        self.feature_thresholds = nn.Parameter(\n",
    "            torch.full([num_trees, depth], float('nan'), dtype=torch.float32), requires_grad=True\n",
    "        )  # nan values will be initialized on first batch (data-aware init)\n",
    "\n",
    "        self.log_temperatures = nn.Parameter(\n",
    "            torch.full([num_trees, depth], float('nan'), dtype=torch.float32), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # binary codes for mapping between 1-hot vectors and bin indices\n",
    "        with torch.no_grad():\n",
    "            indices = torch.arange(2 ** self.depth)\n",
    "            offsets = 2 ** torch.arange(self.depth)\n",
    "            bin_codes = (indices.view(1, -1) // offsets.view(-1, 1) % 2).to(torch.float32)\n",
    "            bin_codes_1hot = torch.stack([bin_codes, 1.0 - bin_codes], dim=-1)\n",
    "            self.bin_codes_1hot = nn.Parameter(bin_codes_1hot, requires_grad=False)\n",
    "            # ^-- [depth, 2 ** depth, 2]\n",
    "\n",
    "    def forward(self, input):\n",
    "        assert len(input.shape) >= 2\n",
    "        if len(input.shape) > 2:\n",
    "            return self.forward(input.view(-1, input.shape[-1])).view(*input.shape[:-1], -1)\n",
    "        # new input shape: [batch_size, in_features]\n",
    "\n",
    "        feature_logits = self.feature_selection_logits\n",
    "        feature_selectors = self.choice_function(feature_logits, dim=0)\n",
    "        # ^--[in_features, num_trees, depth]\n",
    "\n",
    "        feature_values = torch.einsum('bi,ind->bnd', input, feature_selectors)\n",
    "        # ^--[batch_size, num_trees, depth]\n",
    "\n",
    "        threshold_logits = (feature_values - self.feature_thresholds) * torch.exp(-self.log_temperatures)\n",
    "\n",
    "        threshold_logits = torch.stack([-threshold_logits, threshold_logits], dim=-1)\n",
    "        # ^--[batch_size, num_trees, depth, 2]\n",
    "\n",
    "        bins = self.bin_function(threshold_logits)\n",
    "        # ^--[batch_size, num_trees, depth, 2], approximately binary\n",
    "\n",
    "        bin_matches = torch.einsum('btds,dcs->btdc', bins, self.bin_codes_1hot)\n",
    "        # ^--[batch_size, num_trees, depth, 2 ** depth]\n",
    "\n",
    "        response_weights = torch.prod(bin_matches, dim=-2)\n",
    "        # ^-- [batch_size, num_trees, 2 ** depth]\n",
    "\n",
    "        response = torch.einsum('bnd,ncd->bnc', response_weights, self.response)\n",
    "        # ^-- [batch_size, num_trees, tree_dim]\n",
    "\n",
    "        return response.flatten(1, 2) if self.flatten_output else response\n",
    "\n",
    "    def initialize(self, input, eps=1e-6):\n",
    "        # data-aware initializer\n",
    "        assert len(input.shape) == 2\n",
    "        if input.shape[0] < 1000:\n",
    "            warn(\"Data-aware initialization is performed on less than 1000 data points. This may cause instability.\"\n",
    "                 \"To avoid potential problems, run this model on a data batch with at least 1000 data samples.\"\n",
    "                 \"You can do so manually before training. Use with torch.no_grad() for memory efficiency.\")\n",
    "        with torch.no_grad():\n",
    "            feature_selectors = self.choice_function(self.feature_selection_logits, dim=0)\n",
    "            # ^--[in_features, num_trees, depth]\n",
    "\n",
    "            feature_values = torch.einsum('bi,ind->bnd', input, feature_selectors)\n",
    "            # ^--[batch_size, num_trees, depth]\n",
    "\n",
    "            # initialize thresholds: sample random percentiles of data\n",
    "            percentiles_q = 100 * np.random.beta(self.threshold_init_beta, self.threshold_init_beta,\n",
    "                                                 size=[self.num_trees, self.depth])\n",
    "            self.feature_thresholds.data[...] = torch.as_tensor(\n",
    "                list(map(np.percentile, check_numpy(feature_values.flatten(1, 2).t()), percentiles_q.flatten())),\n",
    "                dtype=feature_values.dtype, device=feature_values.device\n",
    "            ).view(self.num_trees, self.depth)\n",
    "\n",
    "            # init temperatures: make sure enough data points are in the linear region of sparse-sigmoid\n",
    "            temperatures = np.percentile(check_numpy(abs(feature_values - self.feature_thresholds)),\n",
    "                                         q=100 * min(1.0, self.threshold_init_cutoff), axis=0)\n",
    "\n",
    "            # if threshold_init_cutoff > 1, scale everything down by it\n",
    "            temperatures /= max(1.0, self.threshold_init_cutoff)\n",
    "            self.log_temperatures.data[...] = torch.log(torch.as_tensor(temperatures) + eps)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}(in_features={}, num_trees={}, depth={}, tree_dim={}, flatten_output={})\".format(\n",
    "            self.__class__.__name__, self.feature_selection_logits.shape[0],\n",
    "            self.num_trees, self.depth, self.tree_dim, self.flatten_output\n",
    "        )\n",
    "    \n",
    "class DenseBlock(nn.Sequential):\n",
    "    def __init__(self, input_dim, layer_dim, num_layers, tree_dim=1, max_features=None,\n",
    "                 input_dropout=0.0, flatten_output=True, Module=ODST, **kwargs):\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            oddt = Module(input_dim, layer_dim, tree_dim=tree_dim, flatten_output=True, **kwargs)\n",
    "            input_dim = min(input_dim + layer_dim * tree_dim, max_features or float('inf'))\n",
    "            layers.append(oddt)\n",
    "\n",
    "        super().__init__(*layers)\n",
    "        self.num_layers, self.layer_dim, self.tree_dim = num_layers, layer_dim, tree_dim\n",
    "        self.max_features, self.flatten_output = max_features, flatten_output\n",
    "        self.input_dropout = input_dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        initial_features = x.shape[-1]\n",
    "        for layer in self:\n",
    "            layer_inp = x\n",
    "            if self.max_features is not None:\n",
    "                tail_features = min(self.max_features, layer_inp.shape[-1]) - initial_features\n",
    "                if tail_features != 0:\n",
    "                    layer_inp = torch.cat([layer_inp[..., :initial_features], layer_inp[..., -tail_features:]], dim=-1)\n",
    "            if self.training and self.input_dropout:\n",
    "                layer_inp = F.dropout(layer_inp, self.input_dropout)\n",
    "            h = layer(layer_inp)\n",
    "            x = torch.cat([x, h], dim=-1)\n",
    "\n",
    "        outputs = x[..., initial_features:]\n",
    "        if not self.flatten_output:\n",
    "            outputs = outputs.view(*outputs.shape[:-1], self.num_layers * self.layer_dim, self.tree_dim)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    '''\n",
    "    Applies the mish function element-wise:\n",
    "    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
    "    See additional documentation for mish class.\n",
    "    '''\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "def swish(x):\n",
    "    return x * F.sigmoid(x)\n",
    "\n",
    "top_feats = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
    "        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
    "        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
    "        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
    "        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
    "        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
    "        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
    "       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
    "       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
    "       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
    "       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
    "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
    "       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
    "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
    "       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
    "       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
    "       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
    "       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
    "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
    "       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
    "       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
    "       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
    "       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
    "       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
    "       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
    "       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
    "       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
    "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
    "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
    "       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
    "       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
    "       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
    "       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
    "       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
    "       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
    "       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
    "       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
    "       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
    "       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
    "       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
    "       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
    "       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
    "       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
    "       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
    "       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
    "       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
    "       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
    "       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
    "       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
    "       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
    "       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
    "       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
    "       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
    "       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
    "       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
    "       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
    "       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
    "       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
    "       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
    "       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
    "       870, 871, 872, 873, 874]\n",
    "\n",
    "class MoAModel(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, num_targets_nonscored, hidden_size):\n",
    "        super(MoAModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.allfeats_1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features, 1024)),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.allfeats_2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.utils.weight_norm(nn.Linear(1024, 1024)),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        \n",
    "        self.topfeats_1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(len(top_feats)),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.utils.weight_norm(nn.Linear(len(top_feats), 1024)),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.combine_1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024 + 2 + 2 + num_features),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.utils.weight_norm(nn.Linear(1024 + 2 + 2 + num_features, 1024)),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024 + 2 + 2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.utils.weight_norm(nn.Linear(1024 + 2 + 2, num_targets)),\n",
    "        )\n",
    "\n",
    "        self.time_embedding = nn.Embedding(3, 2)\n",
    "        self.dose_embedding = nn.Embedding(2, 2)\n",
    "        \n",
    "        self.target = 0\n",
    "    def forward(self, x):\n",
    "        x_input = x\n",
    "        \n",
    "        #x_topfeats = x[:, top_feats]\n",
    "        \n",
    "        x_time = self.time_embedding(x[:, 0].long())\n",
    "        x_dose = self.dose_embedding(x[:, 1].long())\n",
    "        \n",
    "        x = self.allfeats_1(x)\n",
    "        #x = self.allfeats_2(x)\n",
    "        \n",
    "        #x_topfeats = self.topfeats_1(x_topfeats)\n",
    "        #x = (x + x_topfeats)/2\n",
    "        x = torch.cat([x, x_time, x_dose, x_input], dim=1)\n",
    "        x = self.combine_1(x)\n",
    "        \n",
    "        x = torch.cat([x, x_time, x_dose], dim=1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MoAModelHidden(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, num_targets_nonscored, hidden_size):\n",
    "        super(MoAModelHidden, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features, 1024)),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.utils.weight_norm(nn.Linear(1024, 1024)),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        \n",
    "        self.node = nn.Sequential(nn.BatchNorm1d(num_features), \n",
    "                              DenseBlock(num_features, layer_dim = 8, num_layers = 3, tree_dim = 64, depth = 6, input_dropout = 0.3,\n",
    "                                         flatten_output = True, choice_function = entmax15, bin_function = entmoid15),\n",
    "                             )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024), \n",
    "            nn.Dropout(0.5), \n",
    "            nn.utils.weight_norm(nn.Linear(1024, num_targets)),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #x_input = x\n",
    "        #x_feature = self.features(x_input)\n",
    "        #x_node = self.node(x_input)\n",
    "        #x = torch.cat([x_feature, x_node], dim=1)\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "class LogitsLogLoss(Metric):\n",
    "    \"\"\"\n",
    "    LogLoss with sigmoid applied\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute LogLoss of predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: np.ndarray\n",
    "            Target matrix or vector\n",
    "        y_score: np.ndarray\n",
    "            Score matrix or vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            float\n",
    "            LogLoss of predictions vs targets.\n",
    "        \"\"\"\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_numpy(y_true, y_pred):\n",
    "    loss = 0\n",
    "    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    #y_pred_clip = y_pred\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n",
    "    return loss / y_pred.shape[1]\n",
    "def sigmoid(logits):\n",
    "    return 1 / (1 + np.exp(-logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "KFoldSeed = 1\n",
    "def test_seed(seed):\n",
    "    kf = MultilabelStratifiedKFold(5, shuffle=True, random_state=seed)\n",
    "    for train_index, valid_index in kf.split(X, y):\n",
    "        y_train = y[train_index]\n",
    "        y_train = np.sum(y_train, axis=0)\n",
    "        if y_train[y_train == 0].any():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "while True:\n",
    "    KFoldSeed = random.randint(1,1000000000)\n",
    "    non_empty = test_seed(KFoldSeed)\n",
    "    print(KFoldSeed, non_empty)\n",
    "    if non_empty:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from autograd import grad\n",
    "import autograd.numpy as anp\n",
    "\n",
    "def log_loss_numpy_grad(y_true, y_pred):\n",
    "    loss = 0\n",
    "    y_pred_clip = anp.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    #y_pred_clip = y_pred\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        loss += - anp.mean(y_true[:, i] * anp.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * anp.log(1 - y_pred_clip[:, i]))\n",
    "    return loss / y_pred.shape[1]\n",
    "\n",
    "def ensemble_with_weights(weights, y_preds):\n",
    "    return np.sum(np.array(y_preds) * np.array(weights).reshape(-1, 1, 1), axis=0)\n",
    "\n",
    "def ensemble_with_weights_grad(weights, y_preds):\n",
    "    return anp.sum(y_preds * weights.reshape(-1, 1, 1), axis=0)\n",
    "\n",
    "def get_best_weights(y_true, y_preds):\n",
    "    y_preds = anp.array(y_preds, dtype='float64')\n",
    "    def optimize_func(weights):\n",
    "        weights, l = weights[:-1], weights[-1]\n",
    "        return log_loss_numpy_grad(y_true, ensemble_with_weights_grad(weights, y_preds)) - l * (anp.sum(weights) - 1)\n",
    "    \n",
    "    \n",
    "    def optimize_func_nograd(weights):\n",
    "        return log_loss_numpy(y_true, ensemble_with_weights(weights, y_preds))\n",
    "    def optimize_func_two(x):\n",
    "        return optimize_func_nograd([x, 1-x])\n",
    "    \n",
    "    grad_L = grad(optimize_func)\n",
    "    \n",
    "    def optimize_func_lagrange(params):\n",
    "        weights, l = params[:-1], params[-1]\n",
    "        dweights = grad_L(params)\n",
    "        \n",
    "        dweights, dl = dweights[:-1], dweights[-1]\n",
    "        \n",
    "        return anp.concatenate([dweights, [anp.sum(weights) - 1]])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if len(y_preds) == 2:\n",
    "        optimized = optimize.minimize_scalar(optimize_func_two, bounds=(0,1), method='bounded')\n",
    "        print(optimized)\n",
    "        return [optimized.x, 1-optimized.x]\n",
    "    \n",
    "    x0 = [1/len(y_preds)] * (len(y_preds) + 1)\n",
    "    \n",
    "    return optimize.fsolve(optimize_func_lagrange, x0)[:-1]\n",
    "    #x0 = [2/3, 1/3]\n",
    "    bounds = [(0,1)] * len(y_preds) + [(0, 1)]\n",
    "    def constr_eq(weights):\n",
    "        return sum(weights) - 1\n",
    "    constraints = {'type': 'eq', 'fun': constr_eq}\n",
    "    \n",
    "    #optimized = optimize.minimize(optimize_func, x0, method='trust-constr', bounds=bounds, constraints=constraints)    \n",
    "    optimized = optimize.minimize(optimize_func, x0, bounds=bounds)\n",
    "    print(optimized)\n",
    "    return optimized.x[:-1]\n",
    "    \n",
    "\n",
    "logloss = nn.BCELoss()\n",
    "def grad_weights(weights, y_true, y_preds):\n",
    "    w1 = torch.tensor(weights, requires_grad=True)\n",
    "    weights = w1.double()\n",
    "    y_preds = torch.tensor(y_preds).double()\n",
    "    predsum = torch.sum(y_preds * weights[:-1].view(-1, 1, 1), dim=0)\n",
    "    \n",
    "    predsum = torch.clamp(predsum, 0, 1)\n",
    "    #print(torch.max(stuff), torch.min(stuff))\n",
    "    loss = (logloss(predsum, torch.tensor(y_true).double())) - weights[-1] * (torch.sum(weights[:-1]) - 1)\n",
    "    loss.backward()\n",
    "    return w1.grad.numpy()\n",
    "\n",
    "def torch_get_best_weights(y_true, y_preds):\n",
    "    def grad_pred(weights):\n",
    "        gradw = grad_weights(weights, y_true, y_preds)\n",
    "        print(weights, gradw)\n",
    "        return list(gradw[:-1]) + [sum(weights[:-1]) - 1]\n",
    "\n",
    "    x0 = [1/len(y_preds)] * (len(y_preds) + 1)\n",
    "    return optimize.fsolve(grad_stuff, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, trainloader, validloader, epochs, lr):\n",
    "    \n",
    "    model.to(device)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "    optimizer = LookaheadAdam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * len(trainloader))\n",
    "    #scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, epochs=epochs, steps_per_epoch=len(trainloader))\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_loss = 1\n",
    "    best_preds = []\n",
    "    t = tqdm(range(epochs))\n",
    "    for epoch in t:\n",
    "        train_loss = train_model(model, optimizer, scheduler, criterion, trainloader)\n",
    "        valid_loss, valid_preds = validate_model(model, criterion, validloader)\n",
    "        t.set_postfix(train_loss=train_loss, valid_loss=valid_loss, lr=scheduler.get_last_lr()[0])\n",
    "        \n",
    "        if math.isnan(valid_loss):\n",
    "            model.load_state_dict(torch.load(\"/best_model.pth\"))\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_preds = valid_preds\n",
    "            torch.save(model.state_dict(), \"/best_model.pth\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"/best_model.pth\"))\n",
    "    \n",
    "    return model, best_loss, best_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LookaheadAdam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3801dad9b100>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mTABNET_MAX_EPOCH\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m tabnet_params = dict(n_d=32, n_a=32, n_steps=1, gamma=1.3,\n\u001b[1;32m----> 4\u001b[1;33m                      \u001b[0mlambda_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLookaheadAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#torch.optim.Adam, #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                      \u001b[0moptimizer_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2e-2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                      \u001b[0mcat_idxs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcat_emb_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LookaheadAdam' is not defined"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/optimo/tabnetregressor-2-0-train-infer/data?\n",
    "TABNET_MAX_EPOCH=200\n",
    "tabnet_params = dict(n_d=32, n_a=32, n_steps=1, gamma=1.3,\n",
    "                     lambda_sparse=0, optimizer_fn=LookaheadAdam, #torch.optim.Adam, #\n",
    "                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "                     cat_idxs=[0,1],cat_emb_dim=1,\n",
    "                     mask_type='entmax',\n",
    "                     scheduler_params=dict(mode=\"min\",\n",
    "                                           patience=5,\n",
    "                                           min_lr=1e-5,\n",
    "                                           factor=0.2,),\n",
    "                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                     verbose=10,\n",
    "                     momentum=0.1,\n",
    "                     #n_independent=1,\n",
    "                     #n_shared=1,\n",
    "                     clip_value=1,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "kf = MultilabelStratifiedKFold(n_splits=7, shuffle=True, random_state=KFoldSeed)\n",
    "models = []\n",
    "all_submodels = []\n",
    "losses = []\n",
    "base_losses = []\n",
    "masks = []\n",
    "y_preds = []\n",
    "tqdm.get_lock().locks = []\n",
    "for train_index, valid_index in kf.split(X, y):\n",
    "    X_train, y_train, y_train_nonscored = X[train_index], y[train_index], y_nonscored[train_index]\n",
    "    X_valid, y_valid, y_valid_nonscored = X[valid_index], y[valid_index], y_nonscored[valid_index]\n",
    "    \n",
    "    train_dataset = MoADataset(X_train, y_train)\n",
    "    train_nonscored_dataset = MoADataset(X_train, y_train_nonscored)\n",
    "    valid_dataset = MoADataset(X_valid, y_valid)\n",
    "    valid_nonscored_dataset = MoADataset(X_valid, y_valid_nonscored)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    trainnonscoredloader = torch.utils.data.DataLoader(train_nonscored_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    validnonscoredloader = torch.utils.data.DataLoader(valid_nonscored_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = MoAModel(\n",
    "        num_features=len(X_train[0]),\n",
    "        num_targets=len(y_train[0]),\n",
    "        num_targets_nonscored=len(y_train_nonscored[0]),\n",
    "        hidden_size=1024,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model, best_loss, best_preds = train_nn(model, trainloader, validloader, 30, 0.01)\n",
    "    \n",
    "    \n",
    "    hiddenmodel = MoAModelHidden(\n",
    "        num_features=len(X_train[0]),\n",
    "        num_targets=len(y_train[0]),\n",
    "        num_targets_nonscored=len(y_train_nonscored[0]),\n",
    "        hidden_size=1024,\n",
    "    )\n",
    "    hiddenmodel.to(device)\n",
    "    with torch.no_grad():\n",
    "        hiddenmodel(torch.tensor(X_train).float().to(device))\n",
    "    hiddenmodel, hidden_loss, hidden_preds = train_nn(hiddenmodel, trainloader, validloader, 30, 0.01)\n",
    "    \n",
    "    tabnetmodel = TabNetRegressor(**tabnet_params)\n",
    "    tabnetmodel.fit(X_train=X_train,\n",
    "              y_train=y_train,\n",
    "              eval_set=[(X_valid, y_valid)],\n",
    "              eval_name = [\"val\"],\n",
    "              eval_metric = [\"logits_ll\"],\n",
    "              max_epochs=TABNET_MAX_EPOCH,\n",
    "              patience=20, batch_size=256, virtual_batch_size=256,\n",
    "              num_workers=1, drop_last=False,\n",
    "              # use binary cross entropy as this is not a regression problem\n",
    "              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n",
    "    \n",
    "    tabnet_preds = sigmoid(tabnetmodel.predict(X_valid))\n",
    "    \n",
    "    weights = get_best_weights(y_valid, [best_preds, hidden_preds, tabnet_preds])\n",
    "    \n",
    "    weighted_loss = log_loss_numpy(y_valid, ensemble_with_weights(weights, [best_preds, hidden_preds, tabnet_preds]))\n",
    "    losses.append(weighted_loss)\n",
    "    print(weights)\n",
    "    print(best_loss, log_loss_numpy(y_valid, best_preds), log_loss_numpy(y_valid, hidden_preds), log_loss_numpy(y_valid, tabnet_preds), weighted_loss)\n",
    "    \n",
    "    \n",
    "    testdataset = MoATestDataset(X_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "    pred1 = test_model(model, testloader)\n",
    "    pred2 = test_model(hiddenmodel, testloader)\n",
    "    pred3 = sigmoid(tabnetmodel.predict(X_test))\n",
    "    y_preds.append(ensemble_with_weights(weights, [pred1, pred2, pred3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_submission[:] = y_test\n",
    "df_sample_submission[df_test_features['cp_type'] == 1] = 0\n",
    "df_sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample_submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
